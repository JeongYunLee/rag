import os
import time
import uuid
from dotenv import load_dotenv
## langsmith
from langsmith import Client
from langchain_teddynote import logging
## OpenAI
from langchain_openai import ChatOpenAI
from langchain.schema import ChatMessage
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.schema import AIMessage, HumanMessage, SystemMessage
from langchain_core.messages import HumanMessage, SystemMessage
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
## ChromaDB
import chromadb
from langchain.vectorstores import Chroma
from langchain_community.vectorstores import Chroma
## LangGraph
from typing import TypedDict
from langgraph.graph import END, StateGraph
from langgraph.checkpoint.memory import MemorySaver
from langchain_core.runnables import RunnableConfig
## Streamlit
import streamlit as st
from langchain_community.chat_message_histories import StreamlitChatMessageHistory
from streamlit_feedback import streamlit_feedback
from langchain_core.tracers import LangChainTracer
from langchain_core.tracers.run_collector import RunCollectorCallbackHandler
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain.callbacks.base import BaseCallbackHandler
from langchain.callbacks.tracers.langchain import wait_for_all_tracers


# .env íŒŒì¼ í™œì„±í™” & API KEY ì„¤ì •
load_dotenv()
openai_api_key = os.getenv('OPENAI_API_KEY')
langchain_api_key = os.getenv('LANGCHAIN_API_KEY')
langchain_endpoint = "https://api.smith.langchain.com"

session_id = ''

if openai_api_key:
    st.session_state["openai_api_key"] = openai_api_key
if langchain_api_key:
    st.session_state["langchain_api_key"] = langchain_api_key

# logging.langsmith("240717") 

#############################################################
########################## Title ############################
#############################################################
st.set_page_config(page_title="Address ChatBot", page_icon="ğŸ¤–")
st.title("Address ChatBot")

##############################################################################################################
################################################Retriever#####################################################
##############################################################################################################
class MultiCollectionRetriever:
    def __init__(self, client, collection_names, embedding_function, search_kwargs={"k": 2}):
        self.collections = [
            Chroma(client=client, collection_name=name, embedding_function=embedding_function)
            for name in collection_names
        ]
        self.search_kwargs = search_kwargs

    def retrieve(self, query):
        results = []
        for collection in self.collections:
            # ê° ì»¬ë ‰ì…˜ì—ì„œ ìœ ì‚¬ë„ ê²€ìƒ‰ ìˆ˜í–‰
            documents_with_scores = collection.similarity_search_with_score(query, **self.search_kwargs)
            results.extend(documents_with_scores)
        
        # ìœ ì‚¬ë„ ì ìˆ˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê²°ê³¼ ì •ë ¬ (scoreê°€ ë†’ì„ìˆ˜ë¡ ìœ ì‚¬ë„ê°€ ë†’ìŒ)
        results.sort(key=lambda x: x[1], reverse=False)

        documents = [(doc, score) for doc, score in results]
        return documents

chroma_client = chromadb.PersistentClient('chroma/')
collection_names = ["csv_files_openai_3072", "49_files_openai_3072"]
embedding = OpenAIEmbeddings(model='text-embedding-3-large') 
multi_retriever = MultiCollectionRetriever(chroma_client, collection_names, embedding)
##############################################################################################################
################################################GraphState####################################################
##############################################################################################################
# GraphState ìƒíƒœë¥¼ ì €ì¥í•˜ëŠ” ìš©ë„
class GraphState(TypedDict):
    question: str  # ì§ˆë¬¸
    context: str  # ë¬¸ì„œì˜ ê²€ìƒ‰ ê²°ê³¼
    answer: str  # llmì´ ìƒì„±í•œ ë‹µë³€
    relevance: str  # ë‹µë³€ì˜ ë¬¸ì„œì— ëŒ€í•œ ê´€ë ¨ì„± (groundness check)
##############################################################################################################
################################################vector Retriever##############################################
##############################################################################################################
def retrieve_document(state: GraphState) -> GraphState:
    # Question ì— ëŒ€í•œ ë¬¸ì„œ ê²€ìƒ‰ì„ retriever ë¡œ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    retrieved_docs = multi_retriever.retrieve(state["question"])
    # ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ context í‚¤ì— ì €ì¥í•©ë‹ˆë‹¤.
    return GraphState(context=retrieved_docs[:2])
##############################################################################################################
################################################Groundness Checker ###########################################
##############################################################################################################
chat = ChatOpenAI(model="gpt-4o", api_key=openai_api_key)

def relevance_message(context, question):
    messages = [
        SystemMessage(content="""
            ë„ˆëŠ” Queryì™€ Documentë¥¼ ë¹„êµí•´ì„œ ['grounded', 'notGrounded', 'notSure'] ì…‹ ì¤‘ í•˜ë‚˜ì˜ ë¼ë²¨ì„ ì¶œë ¥í•˜ëŠ” ëª¨ë¸ì´ì•¼.

            'grounded': Compare the Query and the Document. If the Document includes content that can be used to generate an answer to the Query, output the label 'grounded'.
            'notGrounded': Compare the Query and the Document. If the Document not includes content that can be used to generate an answer to the Query, output the label 'notGrounded'.
            'notSure': Compare the Query and the Document. If you cannot determine whether the Document includes content that can be used to generate an answer to the Query, output the label .notSure'.
            
            ë„ˆì˜ ì¶œë ¥ì€ ë°˜ë“œì‹œ 'grounded', 'notGrounded', 'notSure' ì¤‘ í•˜ë‚˜ì—¬ì•¼ í•´. ë„ì–´ì“°ê¸°ë‚˜ ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ë“± ë‹¤ë¥¸ í˜•ì‹ì´ë‚˜ ì¶”ê°€ì ì¸ ì„¤ëª… ì—†ì´ ì˜¤ì§ í•˜ë‚˜ì˜ ë¼ë²¨ë§Œ ì¶œë ¥í•´ì¤˜.
        """),
        HumanMessage(content=f"""
            [Document]
            {context}

            [Query]
            {question}
        """),
    ]
    return messages

def relevance_check(state: GraphState) -> GraphState:
    messages = relevance_message(state["context"], state["question"])
    response = chat.invoke(messages)
    return GraphState(
        relevance=response.content,
        context=state["context"],
        answer=state["answer"],
        question=state["question"],
    )

def is_relevant(state: GraphState) -> GraphState:
    if state["relevance"] == "grounded":
        return "ê´€ë ¨ì„± O"
    elif state["relevance"] == "notGrounded":
        return "ê´€ë ¨ì„± X"
    elif state["relevance"] == "notSure":
        return "í™•ì¸ë¶ˆê°€"
##############################################################################################################
################################################LLM Answer Maker##############################################
##############################################################################################################

class StreamHandler(BaseCallbackHandler):
    def __init__(self, container, initial_text=""):
        self.container = container
        self.text = initial_text

    def on_llm_new_token(self, token: str, **kwargs) -> None:
        self.text += token
        self.container.markdown(self.text)

stream_handler = StreamHandler(st.empty())
chat = ChatOpenAI(model="gpt-4o", api_key=openai_api_key, streaming=True, callbacks=[StreamingStdOutCallbackHandler()])

def message(context, question):
    messages = [
        SystemMessage(content="""
            ë„ˆëŠ” Documentì˜ ì •ë³´ë¥¼ ë°˜ë“œì‹œ í™œìš©í•´ì„œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì±—ë´‡ì´ì•¼. 
            ì´ë•Œ, ë‹µë³€ì€ Documentì— ì •ë³´ê°€ ìˆì„ ìˆ˜ë„ ìˆê³ , ì—†ì„ ìˆ˜ë„ ìˆì–´. 
            Documentì˜ ì •ë³´ë¡œ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ìˆëŠ” ê²½ìš° í•´ë‹¹ ì •ë³´ë¥¼ í™œìš©í•˜ê³ , ë§Œì•½ Documentì˜ ì •ë³´ë¡œ ë‹µë³€ì„ ìœ ì¶”ì¡°ì°¨ í•  ìˆ˜ ì—†ëŠ” ê²½ìš°, Documentë¥¼ ì°¸ê³ í•˜ì§€ ë§ê³  ê·¸ëƒ¥ ë„ˆê°€ ìƒê°í•œ ë‹µë³€ì„ ìƒì„±í•´ì¤˜.
            ì£¼ì†Œì™€ ê´€ë ¨ëœ ì§ˆë¬¸ì¸ ê²½ìš° ìµœëŒ€í•œ Documentì˜ ë‹µë³€ì„ ê¸°ë°˜ì„ ì°¸ê³ í•´ì£¼ê³ , ê·¸ë ‡ì§€ ì•Šì€ ê²½ìš° ê·¸ëƒ¥ ë„ˆì˜ ì§€ì‹ì„ í™œìš©í•´ì¤˜.
            ë‹µë³€ì—ëŠ” Documentë¼ëŠ” ë‹¨ì–´ë¥¼ ì‚¬ìš©í•˜ì§€ ë§ì•„ì¤˜.
            
            ë‹µë³€ì˜ ëì—ëŠ” ì¶œì²˜ì˜ ì •ë³´ë¥¼ ê¸°ì…í•˜ëŠ”ë°, ì¶œì²˜ëŠ” Documentì˜ 'context'ì— metadataì˜ 'source'ì— íŒŒì¼ê²½ë¡œë¡œ ê¸°ì…ë˜ì–´ ìˆì–´. pdf, csv, md ë“±ì˜ íŒŒì¼ ì´ë¦„ìœ¼ë¡œë§Œ ì¶œì²˜ë¥¼ ê¸°ì…í•´ì£¼ë©´ ë¼.
            ë§Œì•½ ì—¬ëŸ¬ê°œì˜ ì¶œì²˜ê°€ ê¸°ì…ë˜ì–´ ìˆëŠ” ê²½ìš° ëª¨ë‘ ì•Œë ¤ì£¼ê³ , ì¤‘ë³µë˜ëŠ” ê²½ìš° í•˜ë‚˜ë§Œ ê¸°ì…í•´ì¤˜.
            ì´ë•Œ íŒŒì¼ëª…ì˜ í™•ì¥ì(pdf, csv, md ë“±)ëŠ” ê¸°ì…í•˜ì§€ ì•Šì•„ë„ ë¼.
                      
            ë§Œì•½ Documentë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ì„ í•˜ì§€ ì•ŠëŠ” ê²½ìš°, ë„ˆê°€ ì•„ëŠ”ëŒ€ë¡œ ë‹µë³€ì„ í•´ì¤˜. 
        """),
        HumanMessage(content=f"""
            [Document]
            {context}

            [Query]
            {question}
        """),
    ]
    return messages

def llm_answer(state: GraphState) -> GraphState:
    messages = message(state["context"], state["question"])
    response = chat.invoke(messages)
    return GraphState(
        answer=response.content,
        context=state["context"],
        question=state["question"],
    )
##############################################################################################################
################################################Setting Graph Relations#######################################
##############################################################################################################

workflow = StateGraph(GraphState)

# ë…¸ë“œë“¤ì„ ì •ì˜í•©ë‹ˆë‹¤.
workflow.add_node("retrieve", retrieve_document)  # ë‹µë³€ì„ ê²€ìƒ‰í•´ì˜¤ëŠ” ë…¸ë“œë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.
workflow.add_node("llm_answer", llm_answer)  # ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ë…¸ë“œë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.
workflow.add_node("relevance_check", relevance_check)  # ë‹µë³€ì˜ ë¬¸ì„œì— ëŒ€í•œ ê´€ë ¨ì„± ì²´í¬ ë…¸ë“œë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.

workflow.add_edge("retrieve", "relevance_check")  # ê²€ìƒ‰ -> ë‹µë³€

# ì¡°ê±´ë¶€ ì—£ì§€ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.
workflow.add_conditional_edges(
    "relevance_check",  # ê´€ë ¨ì„± ì²´í¬ ë…¸ë“œì—ì„œ ë‚˜ì˜¨ ê²°ê³¼ë¥¼ is_relevant í•¨ìˆ˜ì— ì „ë‹¬í•©ë‹ˆë‹¤.
    is_relevant,
    {
        "ê´€ë ¨ì„± O": "llm_answer",  # ê´€ë ¨ì„±ì´ ìˆìœ¼ë©´ ì¢…ë£Œí•©ë‹ˆë‹¤.
        "ê´€ë ¨ì„± X": "llm_answer",  # ê´€ë ¨ì„±ì´ ì—†ìœ¼ë©´ ë‹¤ì‹œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
        "í™•ì¸ë¶ˆê°€": "llm_answer",  # ê´€ë ¨ì„± ì²´í¬ ê²°ê³¼ê°€ ëª¨í˜¸í•˜ë‹¤ë©´ ë‹¤ì‹œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
    },
)

workflow.add_edge("llm_answer", END)  # ë‹µë³€ -> ì¢…ë£Œ

# ì‹œì‘ì ì„ ì„¤ì •í•©ë‹ˆë‹¤.
workflow.set_entry_point("retrieve")

# ê¸°ë¡ì„ ìœ„í•œ ë©”ëª¨ë¦¬ ì €ì¥ì†Œë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
memory = MemorySaver()

# ê·¸ë˜í”„ë¥¼ ì»´íŒŒì¼í•©ë‹ˆë‹¤.
app = workflow.compile(checkpointer=memory)
##############################################################################################################
##############################################################################################################
##############################################################################################################
if "query" not in st.session_state:
    st.session_state.query = None

reset_history = st.sidebar.button("ëŒ€í™”ë‚´ìš© ì´ˆê¸°í™”", type="primary")

# ë©”ëª¨ë¦¬
msgs = StreamlitChatMessageHistory(key="langchain_messages")


if reset_history:
  msgs.clear()
  st.session_state["last_run"] = None
  st.session_state.messages = []
  st.session_state.query = None

if "messages" not in st.session_state:
  st.session_state["messages"] = []

# ì„¸ì…˜ ìƒíƒœì— ì €ì¥ëœ ëª¨ë“  ë©”ì‹œì§€ ì¶œë ¥
for msg in st.session_state.messages:
    st.chat_message(msg.role).write(msg.content)

# ìœ ì €ì˜ ì…ë ¥ì„ ë°›ì•„ì„œ ëŒ€í™”ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.
if user_input := st.chat_input():
    if st.session_state.query is None:
        st.session_state.query = user_input
    st.session_state.messages.append(ChatMessage(role="user", content=user_input))
    st.chat_message("user").write(user_input)

    # RunnableConfigì™€ GraphStateë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ ìƒì„±
    config = RunnableConfig(recursion_limit=5, configurable={"thread_id": "SELF-RAG"})
    inputs = GraphState(question=user_input)

    # ë‹µë³€ ìƒì„± ë° ì¶œë ¥
    with st.chat_message("assistant"):
        with st.spinner("Thinking..."):
            output = app.invoke(inputs, config=config)
            assistant_response = output["answer"]
            st.markdown(assistant_response)
        st.session_state.messages.append(ChatMessage(role="assistant", content=assistant_response))

    # wait_for_all_tracers()
#     st.session_state.last_run = run_collector.traced_runs[0].id


# @st.cache_data(ttl="2h", show_spinner=False)
# def get_run_url(run_id):
#     time.sleep(1)
#     return client.read_run(run_id).url


# if st.session_state.get("last_run"):
#     run_url = get_run_url(st.session_state.last_run)
#     st.sidebar.markdown(f"[LangSmith ì¶”ì :hammer_and_wrench:]({run_url})")
#     feedback = streamlit_feedback(
#         feedback_type="thumbs",
#         optional_text_label="[Optional] Please provide an explanation",
#         key=f"feedback_{st.session_state.last_run}",
#     )
#     if feedback:
#         scores = {"ğŸ‘": 1, "ğŸ‘": 0}

#         client.create_feedback(
#             st.session_state.last_run,
#             feedback["type"],
#             score=scores[feedback["score"]],
#             value = st.session_state.query,
#             comment=feedback.get("text")
#         )
#         st.toast("í”¼ë“œë°±ì„ ì €ì¥í•˜ì˜€ìŠµë‹ˆë‹¤.!", icon="ğŸ“")
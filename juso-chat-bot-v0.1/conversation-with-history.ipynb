{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/teddylee777/langchain-kr/blob/main/12-RAG/03-Conversation-With-History.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 성공한 듯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# API KEY를 환경변수로 관리하기 위한 설정 파일\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API KEY 정보로드\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith 추적을 시작합니다.\n",
      "[프로젝트명]\n",
      "240722\n"
     ]
    }
   ],
   "source": [
    "# LangSmith 추적을 설정합니다. https://smith.langchain.com\n",
    "# !pip install langchain-teddynote\n",
    "from langchain_teddynote import logging\n",
    "\n",
    "# 프로젝트 이름을 입력합니다.\n",
    "logging.langsmith(\"240722\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PDFPlumberLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.chains.transform import TransformChain\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Collection(name=49_files_openai_3072),\n",
       " Collection(name=12_files_openai_3072),\n",
       " Collection(name=csv_files_openai_3072)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 컬렉션 생성/연결하기\n",
    "import chromadb\n",
    "client = chromadb.PersistentClient('chroma/')\n",
    "client.list_collections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "class MultiCollectionRetriever:\n",
    "    def __init__(self, client, collection_names, embedding_function, search_kwargs={\"k\": 2}):\n",
    "        self.collections = [\n",
    "            Chroma(client=client, collection_name=name, embedding_function=embedding_function)\n",
    "            for name in collection_names\n",
    "        ]\n",
    "        self.search_kwargs = search_kwargs\n",
    "\n",
    "    def retrieve(self, query):\n",
    "        results = []\n",
    "        for collection in self.collections:\n",
    "            # 각 컬렉션에서 유사도 검색 수행\n",
    "            documents_with_scores = collection.similarity_search_with_score(query, **self.search_kwargs)\n",
    "            results.extend(documents_with_scores)\n",
    "        \n",
    "        # 유사도 점수를 기준으로 결과 정렬 (score가 높을수록 유사도가 높음)\n",
    "        results.sort(key=lambda x: x[1], reverse=False)\n",
    "\n",
    "        documents = [(doc, score) for doc, score in results]\n",
    "        return documents\n",
    "\n",
    "# 사용 예시\n",
    "client = chromadb.PersistentClient('chroma/')\n",
    "collection_names = [\"csv_files_openai_3072\", \"49_files_openai_3072\"]\n",
    "embedding = OpenAIEmbeddings(model='text-embedding-3-large') \n",
    "multi_retriever = MultiCollectionRetriever(client, collection_names, embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 프롬프트를 생성합니다.\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "            너는 Document의 정보를 반드시 활용해서 답변을 생성하는 챗봇이야. \n",
    "            이때, 답변은 Document에 정보가 있을 수도 있고, 없을 수도 있어. \n",
    "            Document의 정보로 답변을 생성할 수 있는 경우 해당 정보를 활용하고, 만약 Document의 정보로 답변을 유추조차 할 수 없는 경우, Document를 참고하지 말고 그냥 너가 생각한 답변을 생성해줘.\n",
    "            주소와 관련된 질문인 경우 최대한 Document의 답변을 기반을 참고해주고, 그렇지 않은 경우 그냥 너의 지식을 활용해줘.\n",
    "            답변에는 Document라는 단어를 사용하지 말아줘.\n",
    "            \n",
    "            답변의 끝에는 출처의 정보를 기입하는데, 출처는 Document의 'context'에 metadata의 'source'에 파일경로로 기입되어 있어. pdf, csv, md 등의 파일 이름으로만 출처를 기입해주면 돼.\n",
    "            만약 여러개의 출처가 기입되어 있는 경우 모두 알려주고, 중복되는 경우 하나만 기입해줘.\n",
    "            이때 파일명의 확장자(pdf, csv, md 등)는 기입하지 않아도 돼.\n",
    "                      \n",
    "            만약 Document를 기반으로 답변을 하지 않는 경우, 너가 생각한대로 답변을 하괴, 답변의 끝에 작성하는 출처에는 '참고한 문서에는 해당 질문에 답변할 수 있는 내용이 없습니다.' 라고 표기해줘\n",
    "    \n",
    "\n",
    "            #Previous Chat History:\n",
    "            {chat_history}\n",
    "\n",
    "            #Question: \n",
    "            {question} \n",
    "\n",
    "            #Context: \n",
    "            {context} \n",
    "\n",
    "            #Answer:\"\"\"\n",
    "            )\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0)\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": lambda inputs: multi_retriever.retrieve(itemgetter(\"question\")(inputs)),\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "        \"chat_history\": itemgetter(\"chat_history\"),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# 세션 기록을 저장할 딕셔너리\n",
    "store = {}\n",
    "\n",
    "# 세션 ID를 기반으로 세션 기록을 가져오는 함수\n",
    "def get_session_history(session_ids):\n",
    "    # print(f\"[대화 세션ID]: {session_ids}\")\n",
    "    if session_ids not in store:  # 세션 ID가 store에 없는 경우\n",
    "        # 새로운 ChatMessageHistory 객체를 생성하여 store에 저장\n",
    "        store[session_ids] = ChatMessageHistory()\n",
    "    return store[session_ids]  # 해당 세션 ID에 대한 세션 기록 반환\n",
    "\n",
    "\n",
    "# 대화를 기록하는 RAG 체인 생성\n",
    "rag_with_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,  # 세션 기록을 가져오는 함수\n",
    "    input_messages_key=\"question\",  # 사용자의 질문이 템플릿 변수에 들어갈 key\n",
    "    history_messages_key=\"chat_history\",  # 기록 메시지의 키\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 세션 기록을 저장할 딕셔너리\n",
    "# store = {}\n",
    "\n",
    "# # 세션 ID를 기반으로 세션 기록을 가져오는 함수\n",
    "# def get_session_history(session_ids):\n",
    "#     # print(f\"[대화 세션ID]: {session_ids}\")\n",
    "#     if session_ids not in store:  # 세션 ID가 store에 없는 경우\n",
    "#         # 새로운 ChatMessageHistory 객체를 생성하여 store에 저장\n",
    "#         store[session_ids] = ChatMessageHistory()\n",
    "#     return store[session_ids]  # 해당 세션 ID에 대한 세션 기록 반환\n",
    "\n",
    "\n",
    "# # 대화를 기록하는 RAG 체인 생성\n",
    "# rag_with_history = RunnableWithMessageHistory(\n",
    "#     chain,\n",
    "#     get_session_history,  # 세션 기록을 가져오는 함수\n",
    "#     input_messages_key=\"question\",  # 사용자의 질문이 템플릿 변수에 들어갈 key\n",
    "#     history_messages_key=\"chat_history\",  # 기록 메시지의 키\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_with_history(query):\n",
    "    rag_with_history.invoke(\n",
    "    # 질문 입력\n",
    "    {\"question\": query},\n",
    "    # 세션 ID 기준으로 대화를 기록합니다.\n",
    "    config={\"configurable\": {\"session_id\": \"rag123\"}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'invoke'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrag_with_history\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m도로명주소법 1조의 내용은?\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m, in \u001b[0;36mrag_with_history\u001b[0;34m(query)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrag_with_history\u001b[39m(query):\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mrag_with_history\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m(\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# 질문 입력\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: query},\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# 세션 ID 기준으로 대화를 기록합니다.\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     config\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfigurable\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msession_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrag123\u001b[39m\u001b[38;5;124m\"\u001b[39m}})\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'invoke'"
     ]
    }
   ],
   "source": [
    "rag_with_history('도로명주소법 1조의 내용은?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'도로명주소법 제1조는 다음과 같습니다:\\n\\n\"이 법은 도로명주소, 국가기초구역, 국가지점번호 및 사물주소의 표기ㆍ사용ㆍ관리ㆍ활용 등에 관한 사항을 규정함으로써 국민의 생활안전과 편의를 도모하고 관련 산업의 지원을 통하여 국가경쟁력 강화에 이바지함을 목적으로 한다.\"\\n\\n출처: 도로명주소법'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_with_history.invoke(\n",
    "    # 질문 입력\n",
    "    {\"question\": \"도로명주소법 1조의 내용은?\"},\n",
    "    # 세션 ID 기준으로 대화를 기록합니다.\n",
    "    config={\"configurable\": {\"session_id\": \"rag123\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_with_history.invoke(\n",
    "    # 질문 입력\n",
    "    {\"question\": \"도로명주소법 1조의 내용은?\"},\n",
    "    # 세션 ID 기준으로 대화를 기록합니다.\n",
    "    config={\"configurable\": {\"session_id\": \"rag123\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_with_history.invoke(\n",
    "    # 질문 입력\n",
    "    {\"question\": \"2조의 내용은?\"},\n",
    "    # 세션 ID 기준으로 대화를 기록합니다.\n",
    "    config={\"configurable\": {\"session_id\": \"rag123\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 이건 뭐지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PDFPlumberLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from operator import itemgetter\n",
    "\n",
    "# 단계 1: 문서 로드(Load Documents)\n",
    "loader = PDFPlumberLoader(\"data/pdf/도로명주소법(법률)(제17574호)(20210609).pdf\")\n",
    "docs = loader.load()\n",
    "\n",
    "# 단계 2: 문서 분할(Split Documents)\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "split_documents = text_splitter.split_documents(docs)\n",
    "\n",
    "# 단계 3: 임베딩(Embedding) 생성\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# 단계 4: DB 생성(Create DB) 및 저장\n",
    "# 벡터스토어를 생성합니다.\n",
    "vectorstore = Chroma(client=client, collection_name=\"49_files_openai_3072\", embedding_function=embedding)\n",
    "# chromadb_retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 1})\n",
    "# vectorstore = FAISS.from_documents(documents=split_documents, embedding=embeddings)\n",
    "\n",
    "# 단계 5: 검색기(Retriever) 생성\n",
    "# 문서에 포함되어 있는 정보를 검색하고 생성합니다.\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# 단계 6: 프롬프트 생성(Create Prompt)\n",
    "# 프롬프트를 생성합니다.\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the question. \n",
    "If you don't know the answer, just say that you don't know. \n",
    "Answer in Korean.\n",
    "\n",
    "#Previous Chat History:\n",
    "{chat_history}\n",
    "\n",
    "#Question: \n",
    "{question} \n",
    "\n",
    "#Context: \n",
    "{context} \n",
    "\n",
    "#Answer:\"\"\"\n",
    ")\n",
    "\n",
    "# 단계 7: 언어모델(LLM) 생성\n",
    "# 모델(LLM) 을 생성합니다.\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0)\n",
    "\n",
    "# 단계 8: 체인(Chain) 생성\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": itemgetter(\"question\") | retriever,\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "        \"chat_history\": itemgetter(\"chat_history\"),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 세션 기록을 저장할 딕셔너리\n",
    "store = {}\n",
    "\n",
    "\n",
    "# 세션 ID를 기반으로 세션 기록을 가져오는 함수\n",
    "def get_session_history(session_ids):\n",
    "    print(f\"[대화 세션ID]: {session_ids}\")\n",
    "    if session_ids not in store:  # 세션 ID가 store에 없는 경우\n",
    "        # 새로운 ChatMessageHistory 객체를 생성하여 store에 저장\n",
    "        store[session_ids] = ChatMessageHistory()\n",
    "    return store[session_ids]  # 해당 세션 ID에 대한 세션 기록 반환\n",
    "\n",
    "\n",
    "# 대화를 기록하는 RAG 체인 생성\n",
    "rag_with_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,  # 세션 기록을 가져오는 함수\n",
    "    input_messages_key=\"question\",  # 사용자의 질문이 템플릿 변수에 들어갈 key\n",
    "    history_messages_key=\"chat_history\",  # 기록 메시지의 키\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_with_history.invoke(\n",
    "    # 질문 입력\n",
    "    {\"question\": \"도로명주소법 1조의 내용은?\"},\n",
    "    # 세션 ID 기준으로 대화를 기록합니다.\n",
    "    config={\"configurable\": {\"session_id\": \"rag123\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langgraph 결합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import uuid\n",
    "from dotenv import load_dotenv\n",
    "## langsmith\n",
    "from langsmith import Client\n",
    "from langchain_teddynote import logging\n",
    "## OpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import ChatMessage\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "## ChromaDB\n",
    "import chromadb\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_community.vectorstores import Chroma\n",
    "## History\n",
    "from operator import itemgetter\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "## LangGraph\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from typing import TypedDict\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "## Streamlit\n",
    "import streamlit as st\n",
    "from langchain_community.chat_message_histories import StreamlitChatMessageHistory\n",
    "from streamlit_feedback import streamlit_feedback\n",
    "from langchain_core.tracers import LangChainTracer\n",
    "from langchain_core.tracers.run_collector import RunCollectorCallbackHandler\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "from langchain.callbacks.tracers.langchain import wait_for_all_tracers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .env 파일 활성화 & API KEY 설정\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "langchain_api_key = os.getenv('LANGCHAIN_API_KEY')\n",
    "langchain_endpoint = \"https://api.smith.langchain.com\"\n",
    "\n",
    "session_id = ''\n",
    "\n",
    "if openai_api_key:\n",
    "    st.session_state[\"openai_api_key\"] = openai_api_key\n",
    "if langchain_api_key:\n",
    "    st.session_state[\"langchain_api_key\"] = langchain_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################################################\n",
    "################################################Retriever#####################################################\n",
    "##############################################################################################################\n",
    "class MultiCollectionRetriever:\n",
    "    def __init__(self, client, collection_names, embedding_function, search_kwargs={\"k\": 2}):\n",
    "        self.collections = [\n",
    "            Chroma(client=client, collection_name=name, embedding_function=embedding_function)\n",
    "            for name in collection_names\n",
    "        ]\n",
    "        self.search_kwargs = search_kwargs\n",
    "\n",
    "    def retrieve(self, query):\n",
    "        results = []\n",
    "        for collection in self.collections:\n",
    "            # 각 컬렉션에서 유사도 검색 수행\n",
    "            documents_with_scores = collection.similarity_search_with_score(query, **self.search_kwargs)\n",
    "            results.extend(documents_with_scores)\n",
    "        \n",
    "        # 유사도 점수를 기준으로 결과 정렬 (score가 높을수록 유사도가 높음)\n",
    "        results.sort(key=lambda x: x[1], reverse=False)\n",
    "\n",
    "        documents = [(doc, score) for doc, score in results]\n",
    "        return documents\n",
    "\n",
    "# 사용 예시\n",
    "client = chromadb.PersistentClient('chroma/')\n",
    "collection_names = [\"csv_files_openai_3072\", \"49_files_openai_3072\"]\n",
    "embedding = OpenAIEmbeddings(model='text-embedding-3-large') \n",
    "multi_retriever = MultiCollectionRetriever(client, collection_names, embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################################################\n",
    "################################################GraphState####################################################\n",
    "##############################################################################################################\n",
    "# GraphState 상태를 저장하는 용도\n",
    "class GraphState(TypedDict):\n",
    "    question: str  # 질문\n",
    "    context: str  # 문서의 검색 결과\n",
    "    answer: str  # llm이 생성한 답변\n",
    "    relevance: str  # 답변의 문서에 대한 관련성 (groundness check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################################################\n",
    "################################################vector Retriever##############################################\n",
    "##############################################################################################################\n",
    "def retrieve_document(state: GraphState) -> GraphState:\n",
    "    # Question 에 대한 문서 검색을 retriever 로 수행합니다.\n",
    "    retrieved_docs = multi_retriever.retrieve(state[\"question\"])\n",
    "    # 검색된 문서를 context 키에 저장합니다.\n",
    "    return GraphState(context=retrieved_docs[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################################################\n",
    "################################################Groundness Checker ###########################################\n",
    "##############################################################################################################\n",
    "chat = ChatOpenAI(model=\"gpt-4o\", api_key=openai_api_key)\n",
    "\n",
    "def relevance_message(context, question):\n",
    "    messages = [\n",
    "        SystemMessage(content=\"\"\"\n",
    "            너는 Query와 Document를 비교해서 ['grounded', 'notGrounded', 'notSure'] 셋 중 하나의 라벨을 출력하는 모델이야.\n",
    "\n",
    "            'grounded': Compare the Query and the Document. If the Document includes content that can be used to generate an answer to the Query, output the label 'grounded'.\n",
    "            'notGrounded': Compare the Query and the Document. If the Document not includes content that can be used to generate an answer to the Query, output the label 'notGrounded'.\n",
    "            'notSure': Compare the Query and the Document. If you cannot determine whether the Document includes content that can be used to generate an answer to the Query, output the label .notSure'.\n",
    "            \n",
    "            너의 출력은 반드시 'grounded', 'notGrounded', 'notSure' 중 하나여야 해. 띄어쓰기나 대소문자 구분 등 다른 형식이나 추가적인 설명 없이 오직 하나의 라벨만 출력해줘.\n",
    "        \"\"\"),\n",
    "        HumanMessage(content=f\"\"\"\n",
    "            [Document]\n",
    "            {context}\n",
    "\n",
    "            [Query]\n",
    "            {question}\n",
    "        \"\"\"),\n",
    "    ]\n",
    "    return messages\n",
    "\n",
    "def relevance_check(state: GraphState) -> GraphState:\n",
    "    messages = relevance_message(state[\"context\"], state[\"question\"])\n",
    "    response = chat.invoke(messages)\n",
    "    return GraphState(\n",
    "        relevance=response.content,\n",
    "        context=state[\"context\"],\n",
    "        answer=state[\"answer\"],\n",
    "        question=state[\"question\"],\n",
    "    )\n",
    "\n",
    "def is_relevant(state: GraphState) -> GraphState:\n",
    "    if state[\"relevance\"] == \"grounded\":\n",
    "        return \"관련성 O\"\n",
    "    elif state[\"relevance\"] == \"notGrounded\":\n",
    "        return \"관련성 X\"\n",
    "    elif state[\"relevance\"] == \"notSure\":\n",
    "        return \"확인불가\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################################################\n",
    "################################################LLM Answer Maker##############################################\n",
    "##############################################################################################################\n",
    "\n",
    "# 프롬프트를 생성합니다.\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "            너는 Document의 정보를 반드시 활용해서 답변을 생성하는 챗봇이야. \n",
    "            이때, 답변은 Document에 정보가 있을 수도 있고, 없을 수도 있어. \n",
    "            Document의 정보로 답변을 생성할 수 있는 경우 해당 정보를 활용하고, 만약 Document의 정보로 답변을 유추조차 할 수 없는 경우, Document를 참고하지 말고 그냥 너가 생각한 답변을 생성해줘.\n",
    "            주소와 관련된 질문인 경우 최대한 Document의 답변을 기반을 참고해주고, 그렇지 않은 경우 그냥 너의 지식을 활용해줘.\n",
    "            답변에는 Document라는 단어를 사용하지 말아줘.\n",
    "            \n",
    "            답변의 끝에는 출처의 정보를 기입하는데, 출처는 Document의 'context'에 metadata의 'source'에 파일경로로 기입되어 있어. pdf, csv, md 등의 파일 이름으로만 출처를 기입해주면 돼.\n",
    "            만약 여러개의 출처가 기입되어 있는 경우 모두 알려주고, 중복되는 경우 하나만 기입해줘.\n",
    "            이때 파일명의 확장자(pdf, csv, md 등)는 기입하지 않아도 돼.\n",
    "                      \n",
    "            만약 Document를 기반으로 답변을 하지 않는 경우, 너가 생각한대로 답변을 하괴, 답변의 끝에 작성하는 출처에는 '참고한 문서에는 해당 질문에 답변할 수 있는 내용이 없습니다.' 라고 표기해줘\n",
    "    \n",
    "\n",
    "            #Previous Chat History:\n",
    "            {chat_history}\n",
    "\n",
    "            #Question: \n",
    "            {question} \n",
    "\n",
    "            #Context: \n",
    "            {context} \n",
    "\n",
    "            #Answer:\"\"\"\n",
    "            )\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0)\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": lambda inputs: multi_retriever.retrieve(itemgetter(\"question\")(inputs)),\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "        \"chat_history\": itemgetter(\"chat_history\"),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# 세션 기록을 저장할 딕셔너리\n",
    "if \"store\" not in st.session_state:\n",
    "    st.session_state[\"store\"] = {}\n",
    "    \n",
    "# store = {}\n",
    "\n",
    "# 세션 ID를 기반으로 세션 기록을 가져오는 함수\n",
    "# def get_session_history(session_ids):\n",
    "#     # print(f\"[대화 세션ID]: {session_ids}\")\n",
    "#     if session_ids not in store:  # 세션 ID가 store에 없는 경우\n",
    "#         # 새로운 ChatMessageHistory 객체를 생성하여 store에 저장\n",
    "#         store[session_ids] = ChatMessageHistory()\n",
    "#     return store[session_ids]  # 해당 세션 ID에 대한 세션 기록 반환\n",
    "\n",
    "def get_session_history(session_ids):\n",
    "    if session_ids not in st.session_state[\"store\"]:  # 세션 ID가 store에 없는 경우\n",
    "        # 새로운 ChatMessageHistory 객체를 생성하여 store에 저장\n",
    "        st.session_state[\"store\"][session_ids] = ChatMessageHistory()\n",
    "    return st.session_state[\"store\"][session_ids]  # 해당 세션 ID에 대한 세션 기록 반환\n",
    "\n",
    "# 대화를 기록하는 RAG 체인 생성\n",
    "rag_with_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,  # 세션 기록을 가져오는 함수\n",
    "    input_messages_key=\"question\",  # 사용자의 질문이 템플릿 변수에 들어갈 key\n",
    "    history_messages_key=\"chat_history\",  # 기록 메시지의 키\n",
    ")\n",
    "\n",
    "\n",
    "def llm_answer(state: GraphState) -> GraphState:\n",
    "    response = rag_with_history.invoke({'question': state[\"question\"]}, config={\"configurable\": {\"session_id\": \"rag123\"}})\n",
    "    return GraphState(\n",
    "        answer=response,\n",
    "        context=state[\"context\"],\n",
    "        question=state[\"question\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################################################\n",
    "################################################Setting Graph Relations#######################################\n",
    "##############################################################################################################\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# 노드들을 정의합니다.\n",
    "workflow.add_node(\"retrieve\", retrieve_document)  # 답변을 검색해오는 노드를 추가합니다.\n",
    "workflow.add_node(\"llm_answer\", llm_answer)  # 답변을 생성하는 노드를 추가합니다.\n",
    "workflow.add_node(\"relevance_check\", relevance_check)  # 답변의 문서에 대한 관련성 체크 노드를 추가합니다.\n",
    "\n",
    "workflow.add_edge(\"retrieve\", \"relevance_check\")  # 검색 -> 답변\n",
    "\n",
    "# 조건부 엣지를 추가합니다.\n",
    "workflow.add_conditional_edges(\n",
    "    \"relevance_check\",  # 관련성 체크 노드에서 나온 결과를 is_relevant 함수에 전달합니다.\n",
    "    is_relevant,\n",
    "    {\n",
    "        \"관련성 O\": \"llm_answer\",  # 관련성이 있으면 종료합니다.\n",
    "        \"관련성 X\": \"llm_answer\",  # 관련성이 없으면 다시 답변을 생성합니다.\n",
    "        \"확인불가\": \"llm_answer\",  # 관련성 체크 결과가 모호하다면 다시 답변을 생성합니다.\n",
    "    },\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"llm_answer\", END)  # 답변 -> 종료\n",
    "\n",
    "# 시작점을 설정합니다.\n",
    "workflow.set_entry_point(\"retrieve\")\n",
    "\n",
    "# 기록을 위한 메모리 저장소를 설정합니다.\n",
    "memory = MemorySaver()\n",
    "\n",
    "# 그래프를 컴파일합니다.\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(\n",
    "        Image(app.get_graph(xray=True).draw_mermaid_png())\n",
    "    )  # 실행 가능한 객체의 그래프를 mermaid 형식의 PNG로 그려서 표시합니다. xray=True는 추가적인 세부 정보를 포함합니다.\n",
    "except:\n",
    "    # 이 부분은 추가적인 의존성이 필요하며 선택적으로 실행됩니다.\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current session state keys: KeysView(<streamlit.runtime.state.session_state_proxy.SessionStateProxy object at 0x11b3cde50>)\n"
     ]
    }
   ],
   "source": [
    "print(\"Current session state keys:\", st.session_state.keys())\n",
    "if \"store\" not in st.session_state:\n",
    "    print(\"Initializing store in session state.\")\n",
    "    st.session_state[\"store\"] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': '도로명주소법 1조의 내용은?',\n",
       " 'context': [(Document(metadata={'source': 'data/final/도로명주소법.txt'}, page_content='도로명주소법\\n[시행 2021. 6. 9.] [법률 제17574호, 2020. 12. 8., 전부개정]\\n행정안전부(주소생활공간과) 044-205-3567\\n\\n제1조(목적) 이 법은 도로명주소, 국가기초구역, 국가지점번호 및 사물주소의 표기ㆍ사용ㆍ관리ㆍ활용 등에 관한 사항을 규정함으로써 국민의 생활안전과 편의를 도모하고 관련 산업의 지원을 통하여 국가경쟁력 강화에 이바지함을 목적으로 한다.'),\n",
       "   0.8625850081443787),\n",
       "  (Document(metadata={'source': 'data/final/도로명주소법시행령.txt'}, page_content='도로명주소법 시행령\\n[시행 2024. 5. 28.] [대통령령 제34533호, 2024. 5. 28., 타법개정]\\n행정안전부(주소생활공간과) 044-205-3567\\n\\n제1조(목적) 이 영은 「도로명주소법」에서 위임된 사항과 그 시행에 필요한 사항을 규정함을 목적으로 한다.\\n\\n제2조(정의) 이 영에서 사용하는 용어의 뜻은 다음과 같다.\\n1. “예비도로명”이란 도로명을 새로 부여하려거나 기존의 도로명을 변경하려는 경우에 임시로 정하는 도로명을 말한다.\\n2. “유사도로명”이란 특정 도로명을 다른 도로명의 일부로 사용하는 경우 특정 도로명과 다른 도로명 모두를 말한다.\\n3. “동일도로명”이란 도로구간이 서로 연결되어 있으면서 그 이름이 같은 도로명을 말한다.\\n4. “종속구간”이란 다음 각 목의 어느 하나에 해당하는 구간으로서 별도로 도로구간으로 설정하지 않고 그 구간에 접해 있는 주된 도로구간에 포함시킨 구간을 말한다.\\n가. 막다른 구간\\n나. 2개의 도로를 연결하는 구간'),\n",
       "   0.8684205412864685)],\n",
       " 'answer': '도로명주소법 제1조는 다음과 같습니다:\\n\\n\"이 법은 도로명주소, 국가기초구역, 국가지점번호 및 사물주소의 표기ㆍ사용ㆍ관리ㆍ활용 등에 관한 사항을 규정함으로써 국민의 생활안전과 편의를 도모하고 관련 산업의 지원을 통하여 국가경쟁력 강화에 이바지함을 목적으로 한다.\"\\n\\n출처: 도로명주소법, 도로명주소법 시행령, 주소용어최종정리_1123 - 업데이트_231123',\n",
       " 'relevance': 'grounded'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = GraphState(question='도로명주소법 1조의 내용은?')\n",
    "config = RunnableConfig(recursion_limit=5, configurable={\"thread_id\": \"SELF-RAG\"})\n",
    "output = app.invoke(inputs, config=config)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'도로명주소법 제1조는 다음과 같습니다:\\n\\n\"이 법은 도로명주소, 국가기초구역, 국가지점번호 및 사물주소의 표기ㆍ사용ㆍ관리ㆍ활용 등에 관한 사항을 규정함으로써 국민의 생활안전과 편의를 도모하고 관련 산업의 지원을 통하여 국가경쟁력 강화에 이바지함을 목적으로 한다.\"\\n\\n출처: 도로명주소법, 도로명주소법 시행령, 주소용어최종정리_1123 - 업데이트_231123'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = GraphState(question='2조의 내용은?')\n",
    "config = RunnableConfig(recursion_limit=5, configurable={\"thread_id\": \"SELF-RAG\"})\n",
    "output = app.invoke(inputs, config=config)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = GraphState(question='도로명주소법 2조의 내용은?')\n",
    "config = RunnableConfig(recursion_limit=5, configurable={\"thread_id\": \"SELF-RAG\"})\n",
    "output = app.invoke(inputs, config=config)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = GraphState(question='도로명주소법 2조에는 총 몇 개의 용어가 정의되어 있어?')\n",
    "config = RunnableConfig(recursion_limit=5, configurable={\"thread_id\": \"SELF-RAG\"})\n",
    "output = app.invoke(inputs, config=config)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = GraphState(question='여전히 틀렸어')\n",
    "config = RunnableConfig(recursion_limit=5, configurable={\"thread_id\": \"SELF-RAG\"})\n",
    "output = app.invoke(inputs, config=config)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import time\n",
    "# import uuid\n",
    "# from dotenv import load_dotenv\n",
    "# ## langsmith\n",
    "# from langsmith import Client\n",
    "# from langchain_teddynote import logging\n",
    "# ## OpenAI\n",
    "# from langchain_openai import ChatOpenAI\n",
    "# from langchain.schema import ChatMessage\n",
    "# from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "# from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "# from langchain_core.messages import HumanMessage, SystemMessage\n",
    "# from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "# ## ChromaDB\n",
    "# import chromadb\n",
    "# from langchain.vectorstores import Chroma\n",
    "# from langchain_community.vectorstores import Chroma\n",
    "# ## History\n",
    "# from operator import itemgetter\n",
    "# from langchain_core.prompts import PromptTemplate\n",
    "# from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "# ## LangGraph\n",
    "# from langchain_core.output_parsers import StrOutputParser\n",
    "# from typing import TypedDict\n",
    "# from langgraph.graph import END, StateGraph\n",
    "# from langgraph.checkpoint.memory import MemorySaver\n",
    "# from langchain_core.runnables import RunnableConfig\n",
    "# ## Streamlit\n",
    "# import streamlit as st\n",
    "# from langchain_community.chat_message_histories import StreamlitChatMessageHistory\n",
    "# from streamlit_feedback import streamlit_feedback\n",
    "# from langchain_core.tracers import LangChainTracer\n",
    "# from langchain_core.tracers.run_collector import RunCollectorCallbackHandler\n",
    "# from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "# from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "# from langchain.callbacks.base import BaseCallbackHandler\n",
    "# from langchain.callbacks.tracers.langchain import wait_for_all_tracers\n",
    "\n",
    "# # .env 파일 활성화 & API KEY 설정\n",
    "# load_dotenv()\n",
    "# openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "# langchain_api_key = os.getenv('LANGCHAIN_API_KEY')\n",
    "# langchain_endpoint = \"https://api.smith.langchain.com\"\n",
    "\n",
    "# session_id = ''\n",
    "\n",
    "# if openai_api_key:\n",
    "#     st.session_state[\"openai_api_key\"] = openai_api_key\n",
    "# if langchain_api_key:\n",
    "#     st.session_state[\"langchain_api_key\"] = langchain_api_key\n",
    "    \n",
    "# ##############################################################################################################\n",
    "# ################################################Retriever#####################################################\n",
    "# ##############################################################################################################\n",
    "# class MultiCollectionRetriever:\n",
    "#     def __init__(self, client, collection_names, embedding_function, search_kwargs={\"k\": 2}):\n",
    "#         self.collections = [\n",
    "#             Chroma(client=client, collection_name=name, embedding_function=embedding_function)\n",
    "#             for name in collection_names\n",
    "#         ]\n",
    "#         self.search_kwargs = search_kwargs\n",
    "\n",
    "#     def retrieve(self, query):\n",
    "#         results = []\n",
    "#         for collection in self.collections:\n",
    "#             # 각 컬렉션에서 유사도 검색 수행\n",
    "#             documents_with_scores = collection.similarity_search_with_score(query, **self.search_kwargs)\n",
    "#             results.extend(documents_with_scores)\n",
    "        \n",
    "#         # 유사도 점수를 기준으로 결과 정렬 (score가 높을수록 유사도가 높음)\n",
    "#         results.sort(key=lambda x: x[1], reverse=False)\n",
    "\n",
    "#         documents = [(doc, score) for doc, score in results]\n",
    "#         return documents\n",
    "\n",
    "# # 사용 예시\n",
    "# client = chromadb.PersistentClient('chroma/')\n",
    "# collection_names = [\"csv_files_openai_3072\", \"49_files_openai_3072\"]\n",
    "# embedding = OpenAIEmbeddings(model='text-embedding-3-large') \n",
    "# multi_retriever = MultiCollectionRetriever(client, collection_names, embedding)\n",
    "\n",
    "# ##############################################################################################################\n",
    "# ################################################GraphState####################################################\n",
    "# ##############################################################################################################\n",
    "# # GraphState 상태를 저장하는 용도\n",
    "# class GraphState(TypedDict):\n",
    "#     question: str  # 질문\n",
    "#     context: str  # 문서의 검색 결과\n",
    "#     answer: str  # llm이 생성한 답변\n",
    "#     relevance: str  # 답변의 문서에 대한 관련성 (groundness check)\n",
    "    \n",
    "# ##############################################################################################################\n",
    "# ################################################vector Retriever##############################################\n",
    "# ##############################################################################################################\n",
    "# def retrieve_document(state: GraphState) -> GraphState:\n",
    "#     # Question 에 대한 문서 검색을 retriever 로 수행합니다.\n",
    "#     retrieved_docs = multi_retriever.retrieve(state[\"question\"])\n",
    "#     # 검색된 문서를 context 키에 저장합니다.\n",
    "#     return GraphState(context=retrieved_docs[:2])\n",
    "\n",
    "# ##############################################################################################################\n",
    "# ################################################Groundness Checker ###########################################\n",
    "# ##############################################################################################################\n",
    "# chat = ChatOpenAI(model=\"gpt-4o\", api_key=openai_api_key)\n",
    "\n",
    "# def relevance_message(context, question):\n",
    "#     messages = [\n",
    "#         SystemMessage(content=\"\"\"\n",
    "#             너는 Query와 Document를 비교해서 ['grounded', 'notGrounded', 'notSure'] 셋 중 하나의 라벨을 출력하는 모델이야.\n",
    "\n",
    "#             'grounded': Compare the Query and the Document. If the Document includes content that can be used to generate an answer to the Query, output the label 'grounded'.\n",
    "#             'notGrounded': Compare the Query and the Document. If the Document not includes content that can be used to generate an answer to the Query, output the label 'notGrounded'.\n",
    "#             'notSure': Compare the Query and the Document. If you cannot determine whether the Document includes content that can be used to generate an answer to the Query, output the label .notSure'.\n",
    "            \n",
    "#             너의 출력은 반드시 'grounded', 'notGrounded', 'notSure' 중 하나여야 해. 띄어쓰기나 대소문자 구분 등 다른 형식이나 추가적인 설명 없이 오직 하나의 라벨만 출력해줘.\n",
    "#         \"\"\"),\n",
    "#         HumanMessage(content=f\"\"\"\n",
    "#             [Document]\n",
    "#             {context}\n",
    "\n",
    "#             [Query]\n",
    "#             {question}\n",
    "#         \"\"\"),\n",
    "#     ]\n",
    "#     return messages\n",
    "\n",
    "# def relevance_check(state: GraphState) -> GraphState:\n",
    "#     messages = relevance_message(state[\"context\"], state[\"question\"])\n",
    "#     response = chat.invoke(messages)\n",
    "#     return GraphState(\n",
    "#         relevance=response.content,\n",
    "#         context=state[\"context\"],\n",
    "#         answer=state[\"answer\"],\n",
    "#         question=state[\"question\"],\n",
    "#     )\n",
    "\n",
    "# def is_relevant(state: GraphState) -> GraphState:\n",
    "#     if state[\"relevance\"] == \"grounded\":\n",
    "#         return \"관련성 O\"\n",
    "#     elif state[\"relevance\"] == \"notGrounded\":\n",
    "#         return \"관련성 X\"\n",
    "#     elif state[\"relevance\"] == \"notSure\":\n",
    "#         return \"확인불가\"\n",
    "    \n",
    "# ##############################################################################################################\n",
    "# ################################################LLM Answer Maker##############################################\n",
    "# ##############################################################################################################\n",
    "\n",
    "# # 프롬프트를 생성합니다.\n",
    "# prompt = PromptTemplate.from_template(\n",
    "#     \"\"\"\n",
    "#             너는 Document의 정보를 반드시 활용해서 답변을 생성하는 챗봇이야. \n",
    "#             이때, 답변은 Document에 정보가 있을 수도 있고, 없을 수도 있어. \n",
    "#             Document의 정보로 답변을 생성할 수 있는 경우 해당 정보를 활용하고, 만약 Document의 정보로 답변을 유추조차 할 수 없는 경우, Document를 참고하지 말고 그냥 너가 생각한 답변을 생성해줘.\n",
    "#             주소와 관련된 질문인 경우 최대한 Document의 답변을 기반을 참고해주고, 그렇지 않은 경우 그냥 너의 지식을 활용해줘.\n",
    "#             답변에는 Document라는 단어를 사용하지 말아줘.\n",
    "            \n",
    "#             답변의 끝에는 출처의 정보를 기입하는데, 출처는 Document의 'context'에 metadata의 'source'에 파일경로로 기입되어 있어. pdf, csv, md 등의 파일 이름으로만 출처를 기입해주면 돼.\n",
    "#             만약 여러개의 출처가 기입되어 있는 경우 모두 알려주고, 중복되는 경우 하나만 기입해줘.\n",
    "#             이때 파일명의 확장자(pdf, csv, md 등)는 기입하지 않아도 돼.\n",
    "                      \n",
    "#             만약 Document를 기반으로 답변을 하지 않는 경우, 너가 생각한대로 답변을 하괴, 답변의 끝에 작성하는 출처에는 '참고한 문서에는 해당 질문에 답변할 수 있는 내용이 없습니다.' 라고 표기해줘\n",
    "    \n",
    "\n",
    "#             #Previous Chat History:\n",
    "#             {chat_history}\n",
    "\n",
    "#             #Question: \n",
    "#             {question} \n",
    "\n",
    "#             #Context: \n",
    "#             {context} \n",
    "\n",
    "#             #Answer:\"\"\"\n",
    "#             )\n",
    "\n",
    "\n",
    "# llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0)\n",
    "\n",
    "# chain = (\n",
    "#     {\n",
    "#         \"context\": lambda inputs: multi_retriever.retrieve(itemgetter(\"question\")(inputs)),\n",
    "#         \"question\": itemgetter(\"question\"),\n",
    "#         \"chat_history\": itemgetter(\"chat_history\"),\n",
    "#     }\n",
    "#     | prompt\n",
    "#     | llm\n",
    "#     | StrOutputParser()\n",
    "# )\n",
    "\n",
    "# # 세션 기록을 저장할 딕셔너리\n",
    "# if \"store\" not in st.session_state:\n",
    "#     st.session_state[\"store\"] = {}\n",
    "\n",
    "# # 세션 ID를 기반으로 세션 기록을 가져오는 함수\n",
    "# def get_session_history(session_ids):\n",
    "#     # print(f\"[대화 세션ID]: {session_ids}\")\n",
    "#     if session_ids not in store:  # 세션 ID가 store에 없는 경우\n",
    "#         # 새로운 ChatMessageHistory 객체를 생성하여 store에 저장\n",
    "#         store[session_ids] = ChatMessageHistory()\n",
    "#     return store[session_ids]  # 해당 세션 ID에 대한 세션 기록 반환\n",
    "\n",
    "\n",
    "# # 대화를 기록하는 RAG 체인 생성\n",
    "# rag_with_history = RunnableWithMessageHistory(\n",
    "#     chain,\n",
    "#     get_session_history,  # 세션 기록을 가져오는 함수\n",
    "#     input_messages_key=\"question\",  # 사용자의 질문이 템플릿 변수에 들어갈 key\n",
    "#     history_messages_key=\"chat_history\",  # 기록 메시지의 키\n",
    "# )\n",
    "\n",
    "\n",
    "# def llm_answer(state: GraphState) -> GraphState:\n",
    "#     response = rag_with_history.invoke({'question': state[\"question\"]}, config={\"configurable\": {\"session_id\": \"rag123\"}})\n",
    "#     return GraphState(\n",
    "#         answer=response,\n",
    "#         context=state[\"context\"],\n",
    "#         question=state[\"question\"],\n",
    "#     )\n",
    "    \n",
    "# ##############################################################################################################\n",
    "# ################################################Setting Graph Relations#######################################\n",
    "# ##############################################################################################################\n",
    "\n",
    "# workflow = StateGraph(GraphState)\n",
    "\n",
    "# # 노드들을 정의합니다.\n",
    "# workflow.add_node(\"retrieve\", retrieve_document)  # 답변을 검색해오는 노드를 추가합니다.\n",
    "# workflow.add_node(\"llm_answer\", llm_answer)  # 답변을 생성하는 노드를 추가합니다.\n",
    "# workflow.add_node(\"relevance_check\", relevance_check)  # 답변의 문서에 대한 관련성 체크 노드를 추가합니다.\n",
    "\n",
    "# workflow.add_edge(\"retrieve\", \"relevance_check\")  # 검색 -> 답변\n",
    "\n",
    "# # 조건부 엣지를 추가합니다.\n",
    "# workflow.add_conditional_edges(\n",
    "#     \"relevance_check\",  # 관련성 체크 노드에서 나온 결과를 is_relevant 함수에 전달합니다.\n",
    "#     is_relevant,\n",
    "#     {\n",
    "#         \"관련성 O\": \"llm_answer\",  # 관련성이 있으면 종료합니다.\n",
    "#         \"관련성 X\": \"llm_answer\",  # 관련성이 없으면 다시 답변을 생성합니다.\n",
    "#         \"확인불가\": \"llm_answer\",  # 관련성 체크 결과가 모호하다면 다시 답변을 생성합니다.\n",
    "#     },\n",
    "# )\n",
    "\n",
    "# workflow.add_edge(\"llm_answer\", END)  # 답변 -> 종료\n",
    "\n",
    "# # 시작점을 설정합니다.\n",
    "# workflow.set_entry_point(\"retrieve\")\n",
    "\n",
    "# # 기록을 위한 메모리 저장소를 설정합니다.\n",
    "# memory = MemorySaver()\n",
    "\n",
    "# # 그래프를 컴파일합니다.\n",
    "# app = workflow.compile(checkpointer=memory)\n",
    "\n",
    "# ##############################################################################################################\n",
    "# ##############################################################################################################\n",
    "# ##############################################################################################################\n",
    "# if \"query\" not in st.session_state:\n",
    "#     st.session_state.query = None\n",
    "\n",
    "# reset_history = st.sidebar.button(\"대화내용 초기화\", type=\"primary\")\n",
    "\n",
    "# # 메모리\n",
    "# msgs = StreamlitChatMessageHistory(key=\"langchain_messages\")\n",
    "\n",
    "\n",
    "# if reset_history:\n",
    "#   msgs.clear()\n",
    "#   st.session_state[\"last_run\"] = None\n",
    "#   st.session_state.messages = []\n",
    "#   st.session_state.query = None\n",
    "#   st.session_state.store = {}\n",
    "\n",
    "# if \"messages\" not in st.session_state:\n",
    "#   st.session_state[\"messages\"] = []\n",
    "\n",
    "# # 세션 상태에 저장된 모든 메시지 출력\n",
    "# for msg in st.session_state.messages:\n",
    "#     st.chat_message(msg.role).write(msg.content)\n",
    "\n",
    "# # 유저의 입력을 받아서 대화를 진행합니다.\n",
    "# if user_input := st.chat_input():\n",
    "#     if st.session_state.query is None:\n",
    "#         st.session_state.query = user_input\n",
    "#     st.session_state.messages.append(ChatMessage(role=\"user\", content=user_input))\n",
    "#     st.chat_message(\"user\").write(user_input)\n",
    "\n",
    "#     # RunnableConfig와 GraphState를 사용하여 답변 생성\n",
    "#     config = RunnableConfig(recursion_limit=5, configurable={\"thread_id\": \"SELF-RAG\"})\n",
    "#     inputs = GraphState(question=user_input)\n",
    "\n",
    "#     # 답변 생성 및 출력\n",
    "#     with st.chat_message(\"assistant\"):\n",
    "#         with st.spinner(\"Thinking...\"):\n",
    "#             output = app.invoke(inputs, config=config)\n",
    "#             assistant_response = output[\"answer\"]\n",
    "#             st.markdown(assistant_response)\n",
    "#         st.session_state.messages.append(ChatMessage(role=\"assistant\", content=assistant_response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeongyunl/opt/anaconda3/envs/juso-chatbot/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/jeongyunl/opt/anaconda3/envs/juso-chatbot/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "## langsmith\n",
    "from langsmith import Client\n",
    "## OpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import ChatMessage, AIMessage, HumanMessage, SystemMessage\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "## ChromaDB\n",
    "import chromadb\n",
    "from langchain.vectorstores import Chroma\n",
    "## History\n",
    "from operator import itemgetter\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory, StreamlitChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "## LangGraph\n",
    "from typing import TypedDict\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "## gradio\n",
    "import gradio as gr\n",
    "\n",
    "# .env 파일 활성화 & API KEY 설정\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "langchain_api_key = os.getenv('LANGCHAIN_API_KEY')\n",
    "\n",
    "store = {}\n",
    "\n",
    "# 세션 ID를 기반으로 세션 기록을 가져오는 함수\n",
    "def get_session_history(session_ids):\n",
    "    if session_ids not in store:  # 세션 ID가 store에 없는 경우\n",
    "        # 새로운 ChatMessageHistory 객체를 생성하여 store에 저장\n",
    "        store[session_ids] = ChatMessageHistory()\n",
    "    return store[session_ids]  # 해당 세션 ID에 대한 세션 기록 반환\n",
    "\n",
    "##############################################################################################################\n",
    "################################################Retriever#####################################################\n",
    "##############################################################################################################\n",
    "class MultiCollectionRetriever:\n",
    "    def __init__(self, client, collection_names, embedding_function, search_kwargs={\"k\": 2}):\n",
    "        self.collections = [\n",
    "            Chroma(client=client, collection_name=name, embedding_function=embedding_function)\n",
    "            for name in collection_names\n",
    "        ]\n",
    "        self.search_kwargs = search_kwargs\n",
    "\n",
    "    def retrieve(self, query):\n",
    "        results = []\n",
    "        for collection in self.collections:\n",
    "            documents_with_scores = collection.similarity_search_with_score(query, **self.search_kwargs)\n",
    "            results.extend(documents_with_scores)\n",
    "        \n",
    "        results.sort(key=lambda x: x[1], reverse=False)\n",
    "        documents = [(doc, score) for doc, score in results]\n",
    "        return documents\n",
    "\n",
    "# 사용 예시\n",
    "client = chromadb.PersistentClient('chroma/')\n",
    "collection_names = [\"csv_files_openai_3072\", \"49_files_openai_3072\"]\n",
    "embedding = OpenAIEmbeddings(model='text-embedding-3-large')  # 올바른 모델 이름 사용\n",
    "multi_retriever = MultiCollectionRetriever(client, collection_names, embedding)\n",
    "\n",
    "##############################################################################################################\n",
    "################################################GraphState####################################################\n",
    "##############################################################################################################\n",
    "class GraphState(TypedDict):\n",
    "    question: str  # 질문\n",
    "    context: list  # 문서의 검색 결과\n",
    "    answer: str  # llm이 생성한 답변\n",
    "    relevance: str  # 답변의 문서에 대한 관련성 (groundness check)\n",
    "\n",
    "##############################################################################################################\n",
    "################################################vector Retriever##############################################\n",
    "##############################################################################################################\n",
    "def retrieve_document(state: GraphState) -> GraphState:\n",
    "    retrieved_docs = multi_retriever.retrieve(state[\"question\"])\n",
    "    return GraphState(question=state[\"question\"], context=retrieved_docs[:2], answer=\"\", relevance=\"\")\n",
    "\n",
    "##############################################################################################################\n",
    "################################################Groundness Checker ###########################################\n",
    "##############################################################################################################\n",
    "chat = ChatOpenAI(model=\"gpt-4o\", api_key=openai_api_key)\n",
    "\n",
    "def relevance_message(context, question):\n",
    "    messages = [\n",
    "        SystemMessage(content=\"\"\"\n",
    "            너는 Query와 Document를 비교해서 ['grounded', 'notGrounded', 'notSure'] 셋 중 하나의 라벨을 출력하는 모델이야.\n",
    "\n",
    "            'grounded': Compare the Query and the Document. If the Document includes content that can be used to generate an answer to the Query, output the label 'grounded'.\n",
    "            'notGrounded': Compare the Query and the Document. If the Document not includes content that can be used to generate an answer to the Query, output the label 'notGrounded'.\n",
    "            'notSure': Compare the Query and the Document. If you cannot determine whether the Document includes content that can be used to generate an answer to the Query, output the label 'notSure'.\n",
    "            \n",
    "            너의 출력은 반드시 'grounded', 'notGrounded', 'notSure' 중 하나여야 해. 띄어쓰기나 대소문자 구분 등 다른 형식이나 추가적인 설명 없이 오직 하나의 라벨만 출력해줘.\n",
    "        \"\"\"),\n",
    "        HumanMessage(content=f\"\"\"\n",
    "            [Document]\n",
    "            {context}\n",
    "\n",
    "            [Query]\n",
    "            {question}\n",
    "        \"\"\"),\n",
    "    ]\n",
    "    return messages\n",
    "\n",
    "def relevance_check(state: GraphState) -> GraphState:\n",
    "    messages = relevance_message(state[\"context\"], state[\"question\"])\n",
    "    response = chat(messages)\n",
    "    return GraphState(\n",
    "        relevance=response.content,\n",
    "        context=state[\"context\"],\n",
    "        answer=state[\"answer\"],\n",
    "        question=state[\"question\"],\n",
    "    )\n",
    "\n",
    "def is_relevant(state: GraphState) -> str:\n",
    "    if state[\"relevance\"] == \"grounded\":\n",
    "        return \"관련성 O\"\n",
    "    elif state[\"relevance\"] == \"notGrounded\":\n",
    "        return \"관련성 X\"\n",
    "    elif state[\"relevance\"] == \"notSure\":\n",
    "        return \"확인불가\"\n",
    "\n",
    "##############################################################################################################\n",
    "################################################LLM Answer Maker##############################################\n",
    "##############################################################################################################\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "            너는 Document의 정보를 반드시 활용해서 답변을 생성하는 챗봇이야. \n",
    "            이때, 답변은 Document에 정보가 있을 수도 있고, 없을 수도 있어. \n",
    "            Document의 정보로 답변을 생성할 수 있는 경우 해당 정보를 활용하고, 만약 Document의 정보로 답변을 유추조차 할 수 없는 경우, Document를 참고하지 말고 그냥 너가 생각한 답변을 생성해줘.\n",
    "            주소와 관련된 질문인 경우 최대한 Document의 답변을 기반을 참고해주고, 그렇지 않은 경우 그냥 너의 지식을 활용해줘.\n",
    "            답변에는 Document라는 단어를 사용하지 말아줘.\n",
    "            \n",
    "            답변의 끝에는 출처의 정보를 기입하는데, 출처는 Document의 'context'에 metadata의 'source'에 파일경로로 기입되어 있어. pdf, csv, md 등의 파일 이름으로만 출처를 기입해주면 돼.\n",
    "            만약 여러개의 출처가 기입되어 있는 경우 모두 알려주고, 중복되는 경우 하나만 기입해줘.\n",
    "            이때 파일명의 확장자(pdf, csv, md 등)는 기입하지 않아도 돼.\n",
    "                      \n",
    "            만약 Document를 기반으로 답변을 하지 않는 경우, 너가 생각한대로 답변을 하괴, 답변의 끝에 작성하는 출처에는 '참고한 문서에는 해당 질문에 답변할 수 있는 내용이 없습니다.' 라고 표기해줘\n",
    "    \n",
    "            #Previous Chat History:\n",
    "            {chat_history}\n",
    "\n",
    "            #Question: \n",
    "            {question} \n",
    "\n",
    "            #Context: \n",
    "            {context} \n",
    "\n",
    "            #Answer:\"\"\"\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0)\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": lambda inputs: multi_retriever.retrieve(itemgetter(\"question\")(inputs)),\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "        \"chat_history\": itemgetter(\"chat_history\"),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_with_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,  # 세션 기록을 가져오는 함수\n",
    "    input_messages_key=\"question\",  # 사용자의 질문이 템플릿 변수에 들어갈 key\n",
    "    history_messages_key=\"chat_history\",  # 기록 메시지의 키\n",
    ")\n",
    "\n",
    "def llm_answer(state: GraphState) -> GraphState:\n",
    "    response = rag_with_history.invoke({'question': state[\"question\"]}, config={\"configurable\": {\"session_id\": \"rag123\"}})\n",
    "    return GraphState(\n",
    "        answer=response,\n",
    "        context=state[\"context\"],\n",
    "        question=state[\"question\"],\n",
    "        relevance=state[\"relevance\"]\n",
    "    )\n",
    "\n",
    "##############################################################################################################\n",
    "################################################Setting Graph Relations#######################################\n",
    "##############################################################################################################\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "workflow.add_node(\"retrieve\", retrieve_document)  # 답변을 검색해오는 노드를 추가합니다.\n",
    "workflow.add_node(\"llm_answer\", llm_answer)  # 답변을 생성하는 노드를 추가합니다.\n",
    "workflow.add_node(\"relevance_check\", relevance_check)  # 답변의 문서에 대한 관련성 체크 노드를 추가합니다.\n",
    "\n",
    "workflow.add_edge(\"retrieve\", \"relevance_check\")  # 검색 -> 답변\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"relevance_check\",  # 관련성 체크 노드에서 나온 결과를 is_relevant 함수에 전달합니다.\n",
    "    is_relevant,\n",
    "    {\n",
    "        \"관련성 O\": \"llm_answer\",  # 관련성이 있으면 종료합니다.\n",
    "        \"관련성 X\": \"llm_answer\",  \n",
    "        \"확인불가\": \"llm_answer\",  \n",
    "    },\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"llm_answer\", END)  # 답변 -> 종료\n",
    "\n",
    "workflow.set_entry_point(\"retrieve\")\n",
    "\n",
    "memory = MemorySaver()\n",
    "\n",
    "app = workflow.compile(checkpointer=memory)\n",
    "\n",
    "############################################################################################################\n",
    "############# Gradio 인터페이스 생성 #############################################################################\n",
    "############################################################################################################\n",
    "\n",
    "def chat(query, history=None):\n",
    "    print(query)\n",
    "    if history is None:\n",
    "        history = []\n",
    "    \n",
    "    inputs = GraphState(question=query)\n",
    "    config = RunnableConfig(recursion_limit=5, configurable={\"thread_id\": \"SELF-RAG\"})\n",
    "    output = app.invoke(inputs, config=config)\n",
    "    print(output['answer'])\n",
    "    return output['answer']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeongyunl/opt/anaconda3/envs/juso-chatbot/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/jeongyunl/opt/anaconda3/envs/juso-chatbot/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "## langsmith\n",
    "from langsmith import Client\n",
    "## OpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import ChatMessage, AIMessage, HumanMessage, SystemMessage\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "## ChromaDB\n",
    "import chromadb\n",
    "from langchain.vectorstores import Chroma\n",
    "## History\n",
    "from operator import itemgetter\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory, StreamlitChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "## LangGraph\n",
    "from typing import TypedDict\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "## gradio\n",
    "import gradio as gr\n",
    "\n",
    "# .env 파일 활성화 & API KEY 설정\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "langchain_api_key = os.getenv('LANGCHAIN_API_KEY')\n",
    "langchain_endpoint = \"https://api.smith.langchain.com\"\n",
    "\n",
    "session_id = ''\n",
    "\n",
    "store = {}\n",
    "\n",
    "# 세션 ID를 기반으로 세션 기록을 가져오는 함수\n",
    "def get_session_history(session_ids):\n",
    "    if session_ids not in store:  # 세션 ID가 store에 없는 경우\n",
    "        # 새로운 ChatMessageHistory 객체를 생성하여 store에 저장\n",
    "        store[session_ids] = ChatMessageHistory()\n",
    "    return store[session_ids]  # 해당 세션 ID에 대한 세션 기록 반환\n",
    "    \n",
    "##############################################################################################################\n",
    "################################################Retriever#####################################################\n",
    "##############################################################################################################\n",
    "class MultiCollectionRetriever:\n",
    "    def __init__(self, client, collection_names, embedding_function, search_kwargs={\"k\": 2}):\n",
    "        self.collections = [\n",
    "            Chroma(client=client, collection_name=name, embedding_function=embedding_function)\n",
    "            for name in collection_names\n",
    "        ]\n",
    "        self.search_kwargs = search_kwargs\n",
    "\n",
    "    def retrieve(self, query):\n",
    "        results = []\n",
    "        for collection in self.collections:\n",
    "            # 각 컬렉션에서 유사도 검색 수행\n",
    "            documents_with_scores = collection.similarity_search_with_score(query, **self.search_kwargs)\n",
    "            results.extend(documents_with_scores)\n",
    "        \n",
    "        # 유사도 점수를 기준으로 결과 정렬 (score가 높을수록 유사도가 높음)\n",
    "        results.sort(key=lambda x: x[1], reverse=False)\n",
    "\n",
    "        documents = [(doc, score) for doc, score in results]\n",
    "        return documents\n",
    "\n",
    "# 사용 예시\n",
    "client = chromadb.PersistentClient('chroma/')\n",
    "collection_names = [\"csv_files_openai_3072\", \"49_files_openai_3072\"]\n",
    "embedding = OpenAIEmbeddings(model='text-embedding-3-large') \n",
    "multi_retriever = MultiCollectionRetriever(client, collection_names, embedding)\n",
    "\n",
    "##############################################################################################################\n",
    "################################################GraphState####################################################\n",
    "##############################################################################################################\n",
    "# GraphState 상태를 저장하는 용도\n",
    "class GraphState(TypedDict):\n",
    "    question: str  # 질문\n",
    "    context: str  # 문서의 검색 결과\n",
    "    answer: str  # llm이 생성한 답변\n",
    "    relevance: str  # 답변의 문서에 대한 관련성 (groundness check)\n",
    "    \n",
    "##############################################################################################################\n",
    "################################################vector Retriever##############################################\n",
    "##############################################################################################################\n",
    "def retrieve_document(state: GraphState) -> GraphState:\n",
    "    # Question 에 대한 문서 검색을 retriever 로 수행합니다.\n",
    "    retrieved_docs = multi_retriever.retrieve(state[\"question\"])\n",
    "    # 검색된 문서를 context 키에 저장합니다.\n",
    "    return GraphState(context=retrieved_docs[:2])\n",
    "\n",
    "##############################################################################################################\n",
    "################################################Groundness Checker ###########################################\n",
    "##############################################################################################################\n",
    "chat = ChatOpenAI(model=\"gpt-4o\", api_key=openai_api_key)\n",
    "\n",
    "def relevance_message(context, question):\n",
    "    messages = [\n",
    "        SystemMessage(content=\"\"\"\n",
    "            너는 Query와 Document를 비교해서 ['grounded', 'notGrounded', 'notSure'] 셋 중 하나의 라벨을 출력하는 모델이야.\n",
    "\n",
    "            'grounded': Compare the Query and the Document. If the Document includes content that can be used to generate an answer to the Query, output the label 'grounded'.\n",
    "            'notGrounded': Compare the Query and the Document. If the Document not includes content that can be used to generate an answer to the Query, output the label 'notGrounded'.\n",
    "            'notSure': Compare the Query and the Document. If you cannot determine whether the Document includes content that can be used to generate an answer to the Query, output the label .notSure'.\n",
    "            \n",
    "            너의 출력은 반드시 'grounded', 'notGrounded', 'notSure' 중 하나여야 해. 띄어쓰기나 대소문자 구분 등 다른 형식이나 추가적인 설명 없이 오직 하나의 라벨만 출력해줘.\n",
    "        \"\"\"),\n",
    "        HumanMessage(content=f\"\"\"\n",
    "            [Document]\n",
    "            {context}\n",
    "\n",
    "            [Query]\n",
    "            {question}\n",
    "        \"\"\"),\n",
    "    ]\n",
    "    return messages\n",
    "\n",
    "def relevance_check(state: GraphState) -> GraphState:\n",
    "    messages = relevance_message(state[\"context\"], state[\"question\"])\n",
    "    response = chat.invoke(messages)\n",
    "    return GraphState(\n",
    "        relevance=response.content,\n",
    "        context=state[\"context\"],\n",
    "        answer=state[\"answer\"],\n",
    "        question=state[\"question\"],\n",
    "    )\n",
    "\n",
    "def is_relevant(state: GraphState) -> GraphState:\n",
    "    if state[\"relevance\"] == \"grounded\":\n",
    "        return \"관련성 O\"\n",
    "    elif state[\"relevance\"] == \"notGrounded\":\n",
    "        return \"관련성 X\"\n",
    "    elif state[\"relevance\"] == \"notSure\":\n",
    "        return \"확인불가\"\n",
    "    \n",
    "##############################################################################################################\n",
    "################################################LLM Answer Maker##############################################\n",
    "##############################################################################################################\n",
    "\n",
    "# 프롬프트를 생성합니다.\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "            너는 Document의 정보를 반드시 활용해서 답변을 생성하는 챗봇이야. \n",
    "            이때, 답변은 Document에 정보가 있을 수도 있고, 없을 수도 있어. \n",
    "            Document의 정보로 답변을 생성할 수 있는 경우 해당 정보를 활용하고, 만약 Document의 정보로 답변을 유추조차 할 수 없는 경우, Document를 참고하지 말고 그냥 너가 생각한 답변을 생성해줘.\n",
    "            주소와 관련된 질문인 경우 최대한 Document의 답변을 기반을 참고해주고, 그렇지 않은 경우 그냥 너의 지식을 활용해줘.\n",
    "            답변에는 Document라는 단어를 사용하지 말아줘.\n",
    "            \n",
    "            답변의 끝에는 출처의 정보를 기입하는데, 출처는 Document의 'context'에 metadata의 'source'에 파일경로로 기입되어 있어. pdf, csv, md 등의 파일 이름으로만 출처를 기입해주면 돼.\n",
    "            만약 여러개의 출처가 기입되어 있는 경우 모두 알려주고, 중복되는 경우 하나만 기입해줘.\n",
    "            이때 파일명의 확장자(pdf, csv, md 등)는 기입하지 않아도 돼.\n",
    "                      \n",
    "            만약 Document를 기반으로 답변을 하지 않는 경우, 너가 생각한대로 답변을 하괴, 답변의 끝에 작성하는 출처에는 '참고한 문서에는 해당 질문에 답변할 수 있는 내용이 없습니다.' 라고 표기해줘\n",
    "    \n",
    "\n",
    "            #Previous Chat History:\n",
    "            {chat_history}\n",
    "\n",
    "            #Question: \n",
    "            {question} \n",
    "\n",
    "            #Context: \n",
    "            {context} \n",
    "\n",
    "            #Answer:\"\"\"\n",
    "            )\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0)\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": lambda inputs: multi_retriever.retrieve(itemgetter(\"question\")(inputs)),\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "        \"chat_history\": itemgetter(\"chat_history\"),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 대화를 기록하는 RAG 체인 생성\n",
    "rag_with_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,  # 세션 기록을 가져오는 함수\n",
    "    input_messages_key=\"question\",  # 사용자의 질문이 템플릿 변수에 들어갈 key\n",
    "    history_messages_key=\"chat_history\",  # 기록 메시지의 키\n",
    ")\n",
    "\n",
    "\n",
    "def llm_answer(state: GraphState) -> GraphState:\n",
    "    response = rag_with_history.invoke({'question': state[\"question\"]}, config={\"configurable\": {\"session_id\": \"rag123\"}})\n",
    "    return GraphState(\n",
    "        answer=response,\n",
    "        context=state[\"context\"],\n",
    "        question=state[\"question\"],\n",
    "    )\n",
    "    \n",
    "##############################################################################################################\n",
    "################################################Setting Graph Relations#######################################\n",
    "##############################################################################################################\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# 노드들을 정의합니다.\n",
    "workflow.add_node(\"retrieve\", retrieve_document)  # 답변을 검색해오는 노드를 추가합니다.\n",
    "workflow.add_node(\"llm_answer\", llm_answer)  # 답변을 생성하는 노드를 추가합니다.\n",
    "workflow.add_node(\"relevance_check\", relevance_check)  # 답변의 문서에 대한 관련성 체크 노드를 추가합니다.\n",
    "\n",
    "workflow.add_edge(\"retrieve\", \"relevance_check\")  # 검색 -> 답변\n",
    "\n",
    "# 조건부 엣지를 추가합니다.\n",
    "workflow.add_conditional_edges(\n",
    "    \"relevance_check\",  # 관련성 체크 노드에서 나온 결과를 is_relevant 함수에 전달합니다.\n",
    "    is_relevant,\n",
    "    {\n",
    "        \"관련성 O\": \"llm_answer\",  # 관련성이 있으면 종료합니다.\n",
    "        \"관련성 X\": \"llm_answer\",  # 관련성이 없으면 다시 답변을 생성합니다.\n",
    "        \"확인불가\": \"llm_answer\",  # 관련성 체크 결과가 모호하다면 다시 답변을 생성합니다.\n",
    "    },\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"llm_answer\", END)  # 답변 -> 종료\n",
    "\n",
    "# 시작점을 설정합니다.\n",
    "workflow.set_entry_point(\"retrieve\")\n",
    "\n",
    "# 기록을 위한 메모리 저장소를 설정합니다.\n",
    "memory = MemorySaver()\n",
    "\n",
    "# 그래프를 컴파일합니다.\n",
    "langgraph_bot = workflow.compile(checkpointer=memory)\n",
    "\n",
    "# def chat(query):\n",
    "#     inputs = GraphState(question=query)\n",
    "#     config = RunnableConfig(recursion_limit=5, configurable={\"thread_id\": \"SELF-RAG\"})\n",
    "#     output_generator = langgraph_bot.stream(inputs, config=config)\n",
    "#     output = list(output_generator)\n",
    "#     return output[-1]['llm_answer']['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'생성형 AI 가우스를 만든 회사의 2023년도 매출액에 대한 정보는 제공된 자료에 포함되어 있지 않습니다. \\n\\n참고한 문서에는 해당 질문에 답변할 수 있는 내용이 없습니다.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pprint\n",
    "from langgraph.errors import GraphRecursionError\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "def stream_responses(question):\n",
    "    config = RunnableConfig(\n",
    "        recursion_limit=12, configurable={\"thread_id\": \"CORRECTIVE-SEARCH-RAG\"}\n",
    "    )\n",
    "\n",
    "    # AgentState 객체를 활용하여 질문을 입력합니다.\n",
    "    inputs = GraphState(\n",
    "        question=question\n",
    "    )\n",
    "\n",
    "    # app.stream을 통해 입력된 메시지에 대한 출력을 스트리밍합니다.\n",
    "    try:\n",
    "        # for output in langgraph_bot.stream(inputs, config=config):\n",
    "        #     # 출력된 결과에서 키와 값을 순회합니다.\n",
    "        #     for key, value in output.items():\n",
    "        #         # 노드의 이름과 해당 노드에서 나온 출력을 출력합니다.\n",
    "        #         pprint.pprint(f\"Output from node '{key}':\")\n",
    "        #         pprint.pprint(\"---\")\n",
    "        #         # 출력 값을 예쁘게 출력합니다.\n",
    "        #         pprint.pprint(value, indent=2, width=80, depth=None)\n",
    "        #     # 각 출력 사이에 구분선을 추가합니다.\n",
    "        #     pprint.pprint(\"\\n---\\n\")\n",
    "        output_generator = langgraph_bot.stream(inputs, config=config)\n",
    "        output = list(output_generator)\n",
    "    except GraphRecursionError as e:\n",
    "        pprint.pprint(f\"Recursion limit reached: {e}\")\n",
    "    return output[-1]['llm_answer']['answer']\n",
    "\n",
    "# 사용 예제\n",
    "stream_responses(\"생성형 AI 가우스를 만든 회사의 2023년도 매출액은 얼마인가요?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.errors import GraphRecursionError\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "config = RunnableConfig(\n",
    "    recursion_limit=5, configurable={\"thread_id\": \"CORRECTIVE-SEARCH-RAG\"}\n",
    ")\n",
    "\n",
    "# AgentState 객체를 활용하여 질문을 입력합니다.\n",
    "inputs = GraphState(\n",
    "    question=\"hi\"\n",
    ")\n",
    "\n",
    "output_generator = langgraph_bot.stream(inputs, config=config)\n",
    "output = list(output_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'안녕하세요! 어떻게 도와드릴까요?\\n\\n참고한 문서에는 해당 질문에 답변할 수 있는 내용이 없습니다.'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[-1]['llm_answer']['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 함수 invoke 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 8, 'total_tokens': 17}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-cbe13672-a389-48c1-9e71-9ae5e1ebb1a5-0', usage_metadata={'input_tokens': 8, 'output_tokens': 9, 'total_tokens': 17})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "llm.invoke('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 8, 'total_tokens': 17}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-d9032132-b4d8-4c20-b38d-cceadd8f4ade-0', usage_metadata={'input_tokens': 8, 'output_tokens': 9, 'total_tokens': 17})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def llm_chat(query):\n",
    "    llm = ChatOpenAI()\n",
    "    return llm.invoke(query)\n",
    "\n",
    "llm_chat('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'대한민국의 수도는 서울입니다.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "# template 정의. {country}는 변수로, 이후에 값이 들어갈 자리를 의미\n",
    "template = \"{country}의 수도는 어디인가요?\"\n",
    "\n",
    "# from_template 메소드를 이용하여 PromptTemplate 객체 생성\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "chain = prompt | llm\n",
    "\n",
    "def llm_chat_template(query):\n",
    "    return chain.invoke(query).content\n",
    "\n",
    "llm_chat_template('대한민국')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

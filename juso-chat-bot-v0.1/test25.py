import os
from dotenv import load_dotenv
## langsmith
from langsmith import Client
## OpenAI
from langchain_openai import ChatOpenAI
from langchain.schema import ChatMessage, AIMessage, HumanMessage, SystemMessage
from langchain.embeddings.openai import OpenAIEmbeddings
## ChromaDB
import chromadb
from langchain.vectorstores import Chroma
## History
from operator import itemgetter
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnableConfig
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_community.chat_message_histories import ChatMessageHistory, StreamlitChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory
## LangGraph
from typing import TypedDict
from langgraph.graph import END, StateGraph
from langgraph.checkpoint.memory import MemorySaver
## Streamlit
import streamlit as st

# .env íŒŒì¼ í™œì„±í™” & API KEY ì„¤ì •
load_dotenv()
openai_api_key = os.getenv('OPENAI_API_KEY')
langchain_api_key = os.getenv('LANGCHAIN_API_KEY')

#############################################################
########################## Title ############################
#############################################################
st.set_page_config(page_title="Address ChatBot", page_icon="ğŸ¤–")
st.title("Address ChatBot")
    
# ì²˜ìŒ 1ë²ˆë§Œ ì‹¤í–‰í•˜ê¸° ìœ„í•œ ì½”ë“œ
if "messages" not in st.session_state:
    # ëŒ€í™”ê¸°ë¡ì„ ì €ì¥í•˜ê¸° ìœ„í•œ ìš©ë„ë¡œ ìƒì„±í•œë‹¤.
    st.session_state["messages"] = []

# Chain ì €ì¥ìš©
if "chain" not in st.session_state:
    # ì•„ë¬´ëŸ° íŒŒì¼ì„ ì—…ë¡œë“œ í•˜ì§€ ì•Šì„ ê²½ìš°
    st.session_state["chain"] = None

# ëŒ€í™” ë‚´ìš©ì„ ê¸°ì–µí•˜ê¸° ìœ„í•œ ì €ì¥ì†Œ ìƒì„±
# if "store" not in st.session_state:
#     st.session_state["store"] = {}

# ì‚¬ì´ë“œë°” ìƒì„±
with st.sidebar:
    # ì´ˆê¸°í™” ë²„íŠ¼ ìƒì„±
    clear_btn = st.button("ëŒ€í™” ì´ˆê¸°í™”")


# ì´ì „ ëŒ€í™”ë¥¼ ì¶œë ¥
def print_messages():
    for chat_message in st.session_state["messages"]:
        st.chat_message(chat_message.role).write(chat_message.content)


# ìƒˆë¡œìš´ ë©”ì‹œì§€ë¥¼ ì¶”ê°€
def add_message(role, message):
    st.session_state["messages"].append(ChatMessage(role=role, content=message))


# ì„¸ì…˜ IDë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì„¸ì…˜ ê¸°ë¡ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
# def get_session_history(session_ids):
#     if session_ids not in st.session_state["store"]:  # ì„¸ì…˜ IDê°€ storeì— ì—†ëŠ” ê²½ìš°
#         # ìƒˆë¡œìš´ ChatMessageHistory ê°ì²´ë¥¼ ìƒì„±í•˜ì—¬ storeì— ì €ì¥
#         st.session_state["store"][session_ids] = ChatMessageHistory()
#     return st.session_state["store"][session_ids]  # í•´ë‹¹ ì„¸ì…˜ IDì— ëŒ€í•œ ì„¸ì…˜ ê¸°ë¡ ë°˜í™˜
        
store = {}

# ì„¸ì…˜ IDë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì„¸ì…˜ ê¸°ë¡ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
def get_session_history(session_ids):
    if session_ids not in store: 
        store[session_ids] = ChatMessageHistory()
    return store[session_ids]  

##############################################################################################################
################################################Retriever#####################################################
##############################################################################################################
class MultiCollectionRetriever:
    def __init__(self, client, collection_names, embedding_function, search_kwargs={"k": 2}):
        self.collections = [
            Chroma(client=client, collection_name=name, embedding_function=embedding_function)
            for name in collection_names
        ]
        self.search_kwargs = search_kwargs

    def retrieve(self, query):
        results = []
        for collection in self.collections:
            documents_with_scores = collection.similarity_search_with_score(query, **self.search_kwargs)
            results.extend(documents_with_scores)
        
        results.sort(key=lambda x: x[1], reverse=False)
        documents = [(doc, score) for doc, score in results]
        return documents

# ì‚¬ìš© ì˜ˆì‹œ
client = chromadb.PersistentClient('chroma/')
collection_names = ["csv_files_openai_3072", "49_files_openai_3072"]
embedding = OpenAIEmbeddings(model='text-embedding-3-large')  # ì˜¬ë°”ë¥¸ ëª¨ë¸ ì´ë¦„ ì‚¬ìš©
multi_retriever = MultiCollectionRetriever(client, collection_names, embedding)

##############################################################################################################
################################################GraphState####################################################
##############################################################################################################
class GraphState(TypedDict):
    question: str  # ì§ˆë¬¸
    context: list  # ë¬¸ì„œì˜ ê²€ìƒ‰ ê²°ê³¼
    answer: str  # llmì´ ìƒì„±í•œ ë‹µë³€
    relevance: str  # ë‹µë³€ì˜ ë¬¸ì„œì— ëŒ€í•œ ê´€ë ¨ì„± (groundness check)

##############################################################################################################
################################################vector Retriever##############################################
##############################################################################################################
def retrieve_document(state: GraphState) -> GraphState:
    retrieved_docs = multi_retriever.retrieve(state["question"])
    return GraphState(question=state["question"], context=retrieved_docs[:2], answer="", relevance="")

##############################################################################################################
################################################Groundness Checker ###########################################
##############################################################################################################
chat = ChatOpenAI(model="gpt-4o", api_key=openai_api_key)

def relevance_message(context, question):
    messages = [
        SystemMessage(content="""
            ë„ˆëŠ” Queryì™€ Documentë¥¼ ë¹„êµí•´ì„œ ['grounded', 'notGrounded', 'notSure'] ì…‹ ì¤‘ í•˜ë‚˜ì˜ ë¼ë²¨ì„ ì¶œë ¥í•˜ëŠ” ëª¨ë¸ì´ì•¼.

            'grounded': Compare the Query and the Document. If the Document includes content that can be used to generate an answer to the Query, output the label 'grounded'.
            'notGrounded': Compare the Query and the Document. If the Document not includes content that can be used to generate an answer to the Query, output the label 'notGrounded'.
            'notSure': Compare the Query and the Document. If you cannot determine whether the Document includes content that can be used to generate an answer to the Query, output the label 'notSure'.
            
            ë„ˆì˜ ì¶œë ¥ì€ ë°˜ë“œì‹œ 'grounded', 'notGrounded', 'notSure' ì¤‘ í•˜ë‚˜ì—¬ì•¼ í•´. ë„ì–´ì“°ê¸°ë‚˜ ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ë“± ë‹¤ë¥¸ í˜•ì‹ì´ë‚˜ ì¶”ê°€ì ì¸ ì„¤ëª… ì—†ì´ ì˜¤ì§ í•˜ë‚˜ì˜ ë¼ë²¨ë§Œ ì¶œë ¥í•´ì¤˜.
        """),
        HumanMessage(content=f"""
            [Document]
            {context}

            [Query]
            {question}
        """),
    ]
    return messages

def relevance_check(state: GraphState) -> GraphState:
    messages = relevance_message(state["context"], state["question"])
    response = chat(messages)
    return GraphState(
        relevance=response.content,
        context=state["context"],
        answer=state["answer"],
        question=state["question"],
    )

def is_relevant(state: GraphState) -> str:
    if state["relevance"] == "grounded":
        return "ê´€ë ¨ì„± O"
    elif state["relevance"] == "notGrounded":
        return "ê´€ë ¨ì„± X"
    elif state["relevance"] == "notSure":
        return "í™•ì¸ë¶ˆê°€"

##############################################################################################################
################################################LLM Answer Maker##############################################
##############################################################################################################

prompt = PromptTemplate.from_template(
    """
            ë„ˆëŠ” Documentì˜ ì •ë³´ë¥¼ ë°˜ë“œì‹œ í™œìš©í•´ì„œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì±—ë´‡ì´ì•¼. 
            ì´ë•Œ, ë‹µë³€ì€ Documentì— ì •ë³´ê°€ ìˆì„ ìˆ˜ë„ ìˆê³ , ì—†ì„ ìˆ˜ë„ ìˆì–´. 
            Documentì˜ ì •ë³´ë¡œ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ìˆëŠ” ê²½ìš° í•´ë‹¹ ì •ë³´ë¥¼ í™œìš©í•˜ê³ , ë§Œì•½ Documentì˜ ì •ë³´ë¡œ ë‹µë³€ì„ ìœ ì¶”ì¡°ì°¨ í•  ìˆ˜ ì—†ëŠ” ê²½ìš°, Documentë¥¼ ì°¸ê³ í•˜ì§€ ë§ê³  ê·¸ëƒ¥ ë„ˆê°€ ìƒê°í•œ ë‹µë³€ì„ ìƒì„±í•´ì¤˜.
            ì£¼ì†Œì™€ ê´€ë ¨ëœ ì§ˆë¬¸ì¸ ê²½ìš° ìµœëŒ€í•œ Documentì˜ ë‹µë³€ì„ ê¸°ë°˜ì„ ì°¸ê³ í•´ì£¼ê³ , ê·¸ë ‡ì§€ ì•Šì€ ê²½ìš° ê·¸ëƒ¥ ë„ˆì˜ ì§€ì‹ì„ í™œìš©í•´ì¤˜.
            ë‹µë³€ì—ëŠ” Documentë¼ëŠ” ë‹¨ì–´ë¥¼ ì‚¬ìš©í•˜ì§€ ë§ì•„ì¤˜.
            
            ë‹µë³€ì˜ ëì—ëŠ” ì¶œì²˜ì˜ ì •ë³´ë¥¼ ê¸°ì…í•˜ëŠ”ë°, ì¶œì²˜ëŠ” Documentì˜ 'context'ì— metadataì˜ 'source'ì— íŒŒì¼ê²½ë¡œë¡œ ê¸°ì…ë˜ì–´ ìˆì–´. pdf, csv, md ë“±ì˜ íŒŒì¼ ì´ë¦„ìœ¼ë¡œë§Œ ì¶œì²˜ë¥¼ ê¸°ì…í•´ì£¼ë©´ ë¼.
            ë§Œì•½ ì—¬ëŸ¬ê°œì˜ ì¶œì²˜ê°€ ê¸°ì…ë˜ì–´ ìˆëŠ” ê²½ìš° ëª¨ë‘ ì•Œë ¤ì£¼ê³ , ì¤‘ë³µë˜ëŠ” ê²½ìš° í•˜ë‚˜ë§Œ ê¸°ì…í•´ì¤˜.
            ì´ë•Œ íŒŒì¼ëª…ì˜ í™•ì¥ì(pdf, csv, md ë“±)ëŠ” ê¸°ì…í•˜ì§€ ì•Šì•„ë„ ë¼.
                      
            ë§Œì•½ Documentë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ì„ í•˜ì§€ ì•ŠëŠ” ê²½ìš°, ë„ˆê°€ ìƒê°í•œëŒ€ë¡œ ë‹µë³€ì„ í•˜ê´´, ë‹µë³€ì˜ ëì— ì‘ì„±í•˜ëŠ” ì¶œì²˜ì—ëŠ” 'ì°¸ê³ í•œ ë¬¸ì„œì—ëŠ” í•´ë‹¹ ì§ˆë¬¸ì— ë‹µë³€í•  ìˆ˜ ìˆëŠ” ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤.' ë¼ê³  í‘œê¸°í•´ì¤˜
    
            #Previous Chat History:
            {chat_history}

            #Question: 
            {question} 

            #Context: 
            {context} 

            #Answer:"""
)

llm = ChatOpenAI(model_name="gpt-4o", temperature=0)

chain = (
    {
        "context": lambda inputs: multi_retriever.retrieve(itemgetter("question")(inputs)),
        "question": itemgetter("question"),
        "chat_history": itemgetter("chat_history"),
    }
    | prompt
    | llm
    | StrOutputParser()
)

rag_with_history = RunnableWithMessageHistory(
    chain,
    get_session_history,  # ì„¸ì…˜ ê¸°ë¡ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
    input_messages_key="question",  # ì‚¬ìš©ìì˜ ì§ˆë¬¸ì´ í…œí”Œë¦¿ ë³€ìˆ˜ì— ë“¤ì–´ê°ˆ key
    history_messages_key="chat_history",  # ê¸°ë¡ ë©”ì‹œì§€ì˜ í‚¤
)

def llm_answer(state: GraphState) -> GraphState:
    response = rag_with_history.invoke({'question': state["question"]}, config={"configurable": {"session_id": "rag123"}})
    return GraphState(
        answer=response,
        context=state["context"],
        question=state["question"],
        relevance=state["relevance"]
    )

##############################################################################################################
################################################Setting Graph Relations#######################################
##############################################################################################################

workflow = StateGraph(GraphState)

workflow.add_node("retrieve", retrieve_document)  # ë‹µë³€ì„ ê²€ìƒ‰í•´ì˜¤ëŠ” ë…¸ë“œë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.
workflow.add_node("llm_answer", llm_answer)  # ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ë…¸ë“œë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.
workflow.add_node("relevance_check", relevance_check)  # ë‹µë³€ì˜ ë¬¸ì„œì— ëŒ€í•œ ê´€ë ¨ì„± ì²´í¬ ë…¸ë“œë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.

workflow.add_edge("retrieve", "relevance_check")  # ê²€ìƒ‰ -> ë‹µë³€

workflow.add_conditional_edges(
    "relevance_check",  # ê´€ë ¨ì„± ì²´í¬ ë…¸ë“œì—ì„œ ë‚˜ì˜¨ ê²°ê³¼ë¥¼ is_relevant í•¨ìˆ˜ì— ì „ë‹¬í•©ë‹ˆë‹¤.
    is_relevant,
    {
        "ê´€ë ¨ì„± O": "llm_answer",  # ê´€ë ¨ì„±ì´ ìˆìœ¼ë©´ ì¢…ë£Œí•©ë‹ˆë‹¤.
        "ê´€ë ¨ì„± X": "llm_answer",  
        "í™•ì¸ë¶ˆê°€": "llm_answer",  
    },
)

workflow.add_edge("llm_answer", END)  # ë‹µë³€ -> ì¢…ë£Œ

workflow.set_entry_point("retrieve")

memory = MemorySaver()

app = workflow.compile(checkpointer=memory)

##############################################################################################################
##############################################################################################################
##############################################################################################################

# # ì´ˆê¸°í™” ë²„íŠ¼ì´ ëˆŒë¦¬ë©´...
# if clear_btn:
#     st.session_state["messages"] = []
#     st.session_state["store"] = {}

# # ì´ì „ ëŒ€í™” ê¸°ë¡ ì¶œë ¥
# print_messages()

# ì‚¬ìš©ìì˜ ì…ë ¥
user_input = st.chat_input("ê¶ê¸ˆí•œ ë‚´ìš©ì„ ë¬¼ì–´ë³´ì„¸ìš”!")

# ê²½ê³  ë©”ì‹œì§€ë¥¼ ë„ìš°ê¸° ìœ„í•œ ë¹ˆ ì˜ì—­
warning_msg = st.empty()

if user_input:
    st.chat_message("user").write(user_input)
    # ìŠ¤íŠ¸ë¦¬ë° í˜¸ì¶œ
    config = RunnableConfig(recursion_limit=5, configurable={"thread_id": "SELF-RAG"})
    inputs = GraphState(question=user_input)
    
    with st.chat_message("assistant"):
        # ë¹ˆ ê³µê°„(ì»¨í…Œì´ë„ˆ)ì„ ë§Œë“¤ì–´ì„œ, ì—¬ê¸°ì— í† í°ì„ ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥í•œë‹¤.
        container = st.empty()
        with st.spinner("Thinking..."):        
            response = app.invoke(inputs, config=config)

        ai_answer = ""
        for token in response["answer"]:
            ai_answer += token
            container.markdown(ai_answer)


    # ëŒ€í™”ê¸°ë¡ì„ ì €ì¥í•œë‹¤.
    add_message("user", user_input)
    add_message("assistant", ai_answer)